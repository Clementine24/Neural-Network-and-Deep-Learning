trainer: sketch_transformer
# Training Setting
batch_size: 10
num_iterations: 200000000 # 0.3*
num_epoch: 140
learning_rate: 0.0001
gpu_ids: [0]
task_types: ['maskrec'] # maskrec, maskgmm, maskdisc, sketchcls, sketchclsinput, sketchretrieval, sbir
get_type: 'single' # 'single'
mask_task_type: 'task'

load_pretrained: 'continue' # [scratch,continue pretrained]
which_pretrained: ['enc_net', 'task_net'] #enc_opt
restore_checkpoint_path: 'model_logs/sketch_transformer/202206112346_orc_bert/best_ckpt.pth.tar'
#restore_checkpoint_path: 'pretrained/qd_8_12_768_mask/latest_ckpt.pth.tar'
dataset: 'quickdraw_memmap' #'quickdraw_memmap' #quickdraw_sbir, quickdraw_memmap tuberlin_memmap
num_train_samples: 20000000
num_val_samples: 2000
num_display_samples: 10000
shuffle_val: False
loader_num_workers: 0
sum_path: 'testOracleDataset/memmap_sum_source.txt'
image_sum_path: 'QuickDraw/sbir_image_sum.txt'
offset_path: 'testOracleDataset/offsets_source.npz'
cls_limit_path: ''
mode: 'train'
max_length: 300
max_size: [50, 50]
image_size: 224
type_size: 3
mask_prob: 0.85 #0.85
limit: 1000
stroke_type: 'stroke-5'
input_is_complete: False
max_cls_cache: 345 # quickdraw 345 tuberlin
normalization_type: 'max_scale'
max_scale_factor: 10
each_max_samples: 1000000000
each_image_max_samples: 100
each_val_samples: 100000

# Output and Save options
print_every:  150000
log_dir: 'orc_bert'

checkpoint_every: 300000
save_model_every: 621050

# Transformer settings
encoder_type: 'Ori'
layers_setting: [[8, 128, 512], [8, 128, 512], [8, 128, 512], [8, 128, 512], [8, 128, 512], [8, 128, 512], [8, 128, 512], [8, 128, 512]]
#layers_setting: [[12, 768, 3072], [12, 768, 3072], [12, 768, 3072], [12, 768, 3072], [12, 768, 3072], [12, 768, 3072], [12, 768, 3072], [12, 768, 3072]]
output_attentions: False
output_all_states: False
keep_multihead_output: False
input_dim: 5
cls_dim: 345
latent_dim: 128
rel_feat_dim: 128
M: 16
embed_layers_setting: [64, 128, 128]  # [128,256,512], [64,128], [128,256,512]
#embed_layers_setting: [128,256,512]
rel_layers_setting: []
cls_layers_setting: []
rec_layers_setting: [128, 128, 64]  # [512,256,128], [128,64],[512,256,128]
#rec_layers_setting: [512,256,128]
sketch_embed_type: 'linear'
embed_pool_type: 'sum'
model_type: 'albert'
position_type: 'learn' #'learn'
segment_type: 'none'
atten_type: 'single' #
attention_norm_type: 'LN'
inter_activation: 'gelu'
attention_dropout_prob: 0.5
hidden_dropout_prob: 0.5
output_dropout_prob: 0.5
triplet_margin: 2.0
gamma: 0.1

# Losses weights
mask_gmm_weight: 1
rec_gmm_weight: 0
mask_axis_weight: 1
rec_axis_weight: 0
mask_type_weight: 1
rec_type_weight: 0
prediction_weight: 1
triplet_weight: 1
